{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f1450f",
   "metadata": {},
   "source": [
    "###  Audio Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1583829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  \n",
    "import torch\n",
    "import whisper\n",
    "import easyocr\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "video_path = \"sample.mp4\" \n",
    "audio_path = \"audio.wav\"  \n",
    "output_folder = \"whisper_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068dce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:04.680]  You know AI trends are moving very fast.\n",
      "[00:04.680 --> 00:06.560]  It's almost on a daily basis.\n",
      "[00:06.560 --> 00:09.620]  But I'm here to help your brand to catch the latest trend.\n",
      "[00:09.620 --> 00:11.920]  So let me show you AI Capsule.\n",
      "[00:11.920 --> 00:13.700]  It's an amazing solution.\n",
      "[00:13.700 --> 00:16.560]  So here, as usual, that's the live view.\n",
      "[00:16.560 --> 00:17.560]  They will place themselves.\n",
      "[00:17.560 --> 00:20.880]  Let me take a pose.\n",
      "[00:20.880 --> 00:27.480]  And now the AI will start to, in the back end, transform my photo into the capsule.\n",
      "[00:27.480 --> 00:32.520]  And what's interesting about this capsule during your event is that we can put your\n",
      "[00:32.520 --> 00:38.640]  prospect inside that capsule and inside that capsule will be the product that you are providing\n",
      "[00:38.640 --> 00:39.640]  in the market.\n",
      "[00:39.640 --> 00:41.640]  It can be clothes, it can be makeups.\n",
      "[00:41.640 --> 00:48.140]  All in one capsule means your consumer and your product in one place.\n",
      "[00:48.140 --> 00:53.120]  And check out how is the results.\n",
      "[00:53.280 --> 00:57.680]  And check out how amazing was the results in a few seconds.\n",
      "[00:57.680 --> 01:03.000]  So why always having this avatar, face swap and this kind of things?\n",
      "[01:03.000 --> 01:04.000]  They are cool.\n",
      "[01:04.000 --> 01:09.260]  But what's cooler is to follow the latest trend for your brand to be on the top.\n",
      "[01:09.260 --> 01:11.520]  With AI with me, we'll get your brand noted.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract Audio from Video\n",
    "subprocess.run([\"ffmpeg\", \"-y\",  \"-i\", video_path, \"-ac\", \"1\", \"-ar\", \"16000\", audio_path])\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "\n",
    "# Transcribe returns (segments + timestamps + confidence)\n",
    "result = model.transcribe(audio_path, verbose=True)\n",
    "\n",
    "\n",
    "# apply bg music filter, threshold for Bg music I used is 0.3 confidance \n",
    "filtered_lines = []\n",
    "for segment in result[\"segments\"]:\n",
    "    if segment.get(\"avg_logprob\", 0) > -1.2:  # so the model is using the avg log Probability, so log (0.3) =-1.2 so that is why i have to use its after conversion \n",
    "        start = segment[\"start\"]\n",
    "        end = segment[\"end\"]\n",
    "        text = segment[\"text\"].strip()\n",
    "        timestamp = \"[\" + str(round(start, 2)) + \" - \" + str(round(end, 2)) + \"]\"\n",
    "        filtered_lines.append(timestamp + \" \" + text)\n",
    "\n",
    "\n",
    "output_file = os.path.join(output_folder, \"filtered.txt\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in filtered_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if not filtered_lines:\n",
    "    print(\"No speech detected\")\n",
    "else:\n",
    "    print(f\"{len(filtered_lines)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73e567",
   "metadata": {},
   "source": [
    "###  Extract Unique Frames with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32be5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frames in 1 sec =  23\n",
      "\n",
      "total_frames =  2213\n",
      "\n",
      "Length sec =  96\n",
      "\n",
      "\n",
      "54 unique keyframes \n"
     ]
    }
   ],
   "source": [
    "video_path = \"./sample.mp4\"\n",
    "frame_output_dir = \"keyframes\"\n",
    "\n",
    "clip_model_name = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(\"cpu\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "\n",
    "frames_per_second = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "print()\n",
    "print(\"Frames in 1 sec = \", frames_per_second)\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print()\n",
    "print(\"total_frames = \", total_frames)\n",
    "video_duration = total_frames // frames_per_second  \n",
    "print()\n",
    "print(\"Length sec = \", video_duration)\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "frame_index = 0\n",
    "current_second = 0\n",
    "saved_frames = []\n",
    "previous_clip_embedding = None\n",
    "\n",
    "\n",
    "for current_second in range(video_duration):\n",
    "    frame_position = current_second * frames_per_second\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_position)\n",
    "    success, frame = video_capture.read()\n",
    "\n",
    "    if success == False:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    model_inputs = clip_processor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs = model_inputs.to(\"cpu\")\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**model_inputs)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    # If the frame has a lot of similarities as previous then there is no need to save it \n",
    "    should_save_frame = False\n",
    "\n",
    "    if previous_clip_embedding is None:\n",
    "        should_save_frame = True\n",
    "    else:\n",
    "        similarity_score = torch.nn.functional.cosine_similarity(image_features, previous_clip_embedding)\n",
    "        similarity_score = similarity_score.item()\n",
    "        if similarity_score < 0.85:\n",
    "            should_save_frame = True\n",
    "\n",
    "\n",
    "\n",
    "    if should_save_frame == True:\n",
    "        output_filename = \"frame_\" + str(frame_index).zfill(4) + \".jpg\"\n",
    "        output_path = os.path.join(frame_output_dir, output_filename)\n",
    "        pil_image.save(output_path)\n",
    "        saved_frames.append(output_path)\n",
    "        previous_clip_embedding = image_features\n",
    "        frame_index = frame_index + 1\n",
    "\n",
    "\n",
    "print(f\"{len(saved_frames)} unique keyframes \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2533159",
   "metadata": {},
   "source": [
    "### Captioning Keyframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ae2639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_0000.jpg  :  a city with skyscrapers and cars\n",
      "frame_0001.jpg  :  a man standing in front of a window looking out at the city\n",
      "frame_0002.jpg  :  a man in a white shirt and sunglasses standing in front of a window\n",
      "frame_0003.jpg  :  a man in a white shirt and sunglasses standing in front of a window\n",
      "frame_0004.jpg  :  a man in a white shirt and sunglasses stands in front of a window\n",
      "frame_0005.jpg  :  a man in a white shirt and sunglasses standing in front of a window\n",
      "frame_0006.jpg  :  a man in a white shirt and sunglasses stands in front of a large screen\n",
      "frame_0007.jpg  :  a man in a white shirt is holding a black tablet\n",
      "frame_0008.jpg  :  a man is holding a tablet computer in his hand\n",
      "frame_0009.jpg  :  a large screen with a picture of a man in a suit\n",
      "frame_0010.jpg  :  a person is using a video camera to take pictures\n",
      "frame_0011.jpg  :  a man is using a large screen to see a video\n",
      "frame_0012.jpg  :  a man in a white lab coat is looking at a computer screen\n",
      "frame_0013.jpg  :  a large screen with a picture of a man in a lab\n",
      "frame_0014.jpg  :  a man is pointing at a screen with a picture of a man in a suit\n",
      "frame_0015.jpg  :  a man points at a screen displaying a video game\n",
      "frame_0016.jpg  :  a man is using a touch screen to display a video\n",
      "frame_0017.jpg  :  a man in a white shirt is standing in front of a large screen\n",
      "frame_0018.jpg  :  a man in a white shirt is standing in front of a large screen\n",
      "frame_0019.jpg  :  a man in a white shirt and sunglasses standing in an office\n",
      "frame_0020.jpg  :  a man in a white shirt and sunglasses is standing in an office\n",
      "frame_0021.jpg  :  a man in a white shirt and sunglasses is looking at the camera\n",
      "frame_0022.jpg  :  a man in a white shirt and sunglasses is standing in an office\n",
      "frame_0023.jpg  :  a man in a white shirt and sunglasses is standing in an office\n",
      "frame_0024.jpg  :  a man in a white shirt and sunglasses is standing in front of a computer\n",
      "frame_0025.jpg  :  a man in a white shirt and sunglasses is holding a cigarette\n",
      "frame_0026.jpg  :  a man in a white shirt and sunglasses is holding his hands up\n",
      "frame_0027.jpg  :  a man in a white shirt and sunglasses is holding his hands up to his face\n",
      "frame_0028.jpg  :  a man in a white shirt and sunglasses stands in front of a window\n",
      "frame_0029.jpg  :  a man in a suit is standing in front of a large window\n",
      "frame_0030.jpg  :  a man standing in front of a large window\n",
      "frame_0031.jpg  :  a man standing in front of a large window\n",
      "frame_0032.jpg  :  a person holding a cell phone in their hand\n",
      "frame_0033.jpg  :  a man in a white shirt is pointing at a large screen\n",
      "frame_0034.jpg  :  a man in a white shirt is standing in front of a large screen\n",
      "frame_0035.jpg  :  a man in a white shirt and sunglasses is standing in front of a computer\n",
      "frame_0036.jpg  :  a man in a white shirt and sunglasses is standing in front of a computer\n",
      "frame_0037.jpg  :  a man in a white shirt and glasses is standing in front of a computer\n",
      "frame_0038.jpg  :  a man in a white shirt and sunglasses is standing in an office\n",
      "frame_0039.jpg  :  a man in a white shirt and sunglasses stands in an office\n",
      "frame_0040.jpg  :  a man in a white shirt is standing in front of a window\n",
      "frame_0041.jpg  :  a small house in the middle of a forest at night\n",
      "frame_0042.jpg  :  a large orange egg with the word ' home ' on it\n",
      "frame_0043.jpg  :  a photo of a dog in a tent\n",
      "frame_0044.jpg  :  a small orange box with a small white box inside\n",
      "frame_0045.jpg  :  a person is reflected in a mirror\n",
      "frame_0046.jpg  :  a bottle with the word ' i do ' on it\n",
      "frame_0047.jpg  :  a bottle with a cartoon character inside\n",
      "frame_0048.jpg  :  a blur of a bottle of wine\n",
      "frame_0049.jpg  :  a small toy in a glass ball with the word ione on it\n",
      "frame_0050.jpg  :  a toy in a glass ball with the word ' botme '\n",
      "frame_0051.jpg  :  a black background with a crescent in the middle\n",
      "frame_0052.jpg  :  a black and white logo with the letter o\n",
      "frame_0053.jpg  :  get your brand logo\n"
     ]
    }
   ],
   "source": [
    "frame_folder = \"keyframes\"            \n",
    "output_folder = \"captions\"           \n",
    "output_file = os.path.join(output_folder, \"blip_captions.txt\") \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_safetensors=True)\n",
    "model = model.to(device)\n",
    "\n",
    "captions = []\n",
    "\n",
    "for file_name in sorted(os.listdir(frame_folder)):\n",
    "\n",
    "    if file_name.lower().endswith(\".jpg\"):\n",
    "        image_path = os.path.join(frame_folder, file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, synced_gpus=False)\n",
    "            caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        full_caption = file_name + \": \" + caption\n",
    "        captions.append(full_caption)\n",
    "\n",
    "        print(file_name, \" : \" , caption)\n",
    "\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    for line in captions:\n",
    "        file.write(line + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddfe104",
   "metadata": {},
   "source": [
    "### Sequential Captioning (5 Frame treated as a short clip) used to capture motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CODE CELL THIS INDICATES AN UNSOLVED QUERY IN MY MIND\n",
    "\n",
    "# prompt = f\"summarize: Write a concise YouTube video description based on these captions: {combined}. Focus on the product, service, or main concept shown. Describe its features, benefits, and how it’s used in the scene. Include a call to action like subscribing or visiting a website. Ignore details about people’s appearance unless relevant to the product.\"\n",
    "\n",
    "#  Window 1  A man in a white shirt and sunglasses stands in front of a large screen. Describe the product, service, or main concept.\n",
    "#  Window 2  A man in a white shirt is holding a tablet computer in his hand. A person is using a video camera to take pictures a man is using the computer screen a large screen with a picture of the man in the lab. Focus on the product, service, or main concept shown. Describe its features, benefits, and how it’s used in the scene.\n",
    "#  Window 3  Describe the product, service, or main concept.\n",
    "#  Window 4  A man in a white shirt and sunglasses is standing in front of a computer. He is holding a cigarette and holding his hands up. Explain the product, service, or main concept.\n",
    "#  Window 5  A man in a white shirt and sunglasses stands in front of a window. A person holding a cell phone in their hand is pointing at a large screen. Describe its features, benefits, and how it’s used in the scene.\n",
    "#  Window 6  A man in a white shirt and sunglasses is standing in front of a small house. Focus on the product, service, or main concept shown. Describe its features, benefits, and how it’s used in the scene.\n",
    "#  Window 7  Describe the product, service, or main concept of the product.\n",
    "#  Window 8  Describe the product’s features, benefits, and how it’s used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241e8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window 1: looking out at the city a man in a white shirt and sunglasses standing in front of a window\n",
      "Window 2: a man in a white shirt and sunglasses standing in front of a window\n",
      "Window 3: a person is using a video camera to take pictures\n",
      "Window 4: a man points at a screen displaying a video game\n",
      "Window 5: a man in a white shirt and sunglasses is standing in front of a computer\n",
      "Window 6: a man in a suit is standing in front of a large window\n",
      "Window 7: a man in a white shirt is pointing at a large screen\n",
      "Window 8: A man in a white shirt and sunglasses stands in an office.\n",
      "Window 9: a small house in the middle of a forest at night a large orange egg with the word ' home ' on it\n",
      "Window 10: a person is reflected in a mirror a bottle with the word ' i do ' on it\n",
      "Window 11: a toy in a glass ball with the word ' botme ' a black background with a crescent in the middle\n"
     ]
    }
   ],
   "source": [
    "input_file = \"captions/blip_captions.txt\"\n",
    "output_file = \"captions/window_summaries.txt\"\n",
    "window_size = 5\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    captions = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for start_index in range(0, len(captions), window_size):\n",
    "\n",
    "    window = captions[start_index : start_index + window_size]\n",
    "    combined_text = \"\"\n",
    "\n",
    "    for caption in window:\n",
    "        parts = caption.split(\":\", 1)\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            text_only = parts[1].strip()\n",
    "            combined_text += text_only + \" \"\n",
    "\n",
    "    prompt = \"summarize and don't add word like Summary: in the output: \" + combined_text.strip()\n",
    "\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    input_ids = encoded_input[\"input_ids\"]\n",
    "    attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=100,\n",
    "            num_beams=6,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True,\n",
    "            synced_gpus=False\n",
    "        )\n",
    "\n",
    "    summary_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    window_number = (start_index // window_size) + 1\n",
    "    full_summary = f\"Window {window_number}: {summary_text}\"\n",
    "    summaries.append(full_summary)\n",
    "\n",
    "    print(full_summary)\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    for summary in summaries:\n",
    "        file.write(summary + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8dd9f",
   "metadata": {},
   "source": [
    "### OCR to get the text from frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc1f608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_0000.jpg : No text \n",
      "frame_0001.jpg : #XoxO\n",
      "frame_0002.jpg : #XoxQ\n",
      "frame_0003.jpg : No text \n",
      "frame_0004.jpg : No text \n",
      "frame_0005.jpg : EXOXQ\n",
      "frame_0006.jpg : UOKO | iboothme\n",
      "frame_0007.jpg : boathne | Etee\n",
      "frame_0008.jpg : iboothme\n",
      "frame_0009.jpg : No text \n",
      "frame_0010.jpg : No text \n",
      "frame_0011.jpg : No text \n",
      "frame_0012.jpg : 3\n",
      "frame_0013.jpg : No text \n",
      "frame_0014.jpg : Sed | ibootme\n",
      "frame_0015.jpg : DASP | iboothme | FND\n",
      "frame_0016.jpg : El | SeNd | Iothne\n",
      "frame_0017.jpg : ibooothne\n",
      "frame_0018.jpg : No text \n",
      "frame_0019.jpg : No text \n",
      "frame_0020.jpg : No text \n",
      "frame_0021.jpg : No text \n",
      "frame_0022.jpg : No text \n",
      "frame_0023.jpg : Fiboothme\n",
      "frame_0024.jpg : iboothme\n",
      "frame_0025.jpg : ix\n",
      "frame_0026.jpg : No text \n",
      "frame_0027.jpg : No text \n",
      "frame_0028.jpg : No text \n",
      "frame_0029.jpg : btnn\n",
      "frame_0030.jpg : iboothme\n",
      "frame_0031.jpg : iboothme | Avat | Iboothma | Conntm\n",
      "frame_0032.jpg : Gicims | iboothme\n",
      "frame_0033.jpg : ibothne | Dar | contrm\n",
      "frame_0034.jpg : No text \n",
      "frame_0035.jpg : No text \n",
      "frame_0036.jpg : ~4s | SAID;\n",
      "frame_0037.jpg : GAID\n",
      "frame_0038.jpg : AID\" | MOXO\n",
      "frame_0039.jpg : MOKO\n",
      "frame_0040.jpg : No text \n",
      "frame_0041.jpg : No text \n",
      "frame_0042.jpg : (iboothme | 'GeFVCUR BRAND Noticed\n",
      "frame_0043.jpg : No text \n",
      "frame_0044.jpg : Be\n",
      "frame_0045.jpg : No text \n",
      "frame_0046.jpg : 9mdtoodi | bgjitoV bn618 1uoY t9d\n",
      "frame_0047.jpg : iboothme | Get Your Brand Noticed\n",
      "frame_0048.jpg : No text \n",
      "frame_0049.jpg : 9rdtoodi | bgjitoV br618 1uoY 192\n",
      "frame_0050.jpg : iboothme | Get Your Brand Noticed\n",
      "frame_0051.jpg : No text \n",
      "frame_0052.jpg : @b\n",
      "frame_0053.jpg : 0 | GET YOUR BRAND\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frame_folder = \"keyframes\"                  \n",
    "output_folder = \"ocr\"                    \n",
    "output_file = os.path.join(output_folder, \"all_ocr.txt\") \n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=False)\n",
    "ocr_results = []\n",
    "\n",
    "for file_name in sorted(os.listdir(frame_folder)):\n",
    "    if file_name.lower().endswith(\".jpg\"):\n",
    "        \n",
    "        image_path = os.path.join(frame_folder, file_name)\n",
    "        text_list = reader.readtext(image_path, detail=0) \n",
    "        \n",
    "        # data processing like I am handling white sapaces etc  \n",
    "        cleaned_texts = []\n",
    "        for text in text_list:\n",
    "            if text.strip():\n",
    "                cleaned_texts.append(text.strip())\n",
    "\n",
    "        if cleaned_texts:\n",
    "            final_line = \" | \".join(cleaned_texts)\n",
    "            ocr_results.append(f\"{file_name}: {final_line}\")\n",
    "            print(f\"{file_name} : {final_line}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"{file_name} : No text \")\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    for result_line in ocr_results:\n",
    "        file.write(result_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31580cf",
   "metadata": {},
   "source": [
    "### Merging all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dec4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_path = \"whisper_out/filtered.txt\"\n",
    "captions_path = \"captions/blip_captions.txt\"\n",
    "ocr_path = \"ocr/all_ocr.txt\"\n",
    "summaries_path = \"captions/window_summaries.txt\"\n",
    "output_path = \"facts/facts.txt\"\n",
    "output = open(output_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# transcript \n",
    "output.write(\"\\nTranscript\\n\")\n",
    "file1 = open(transcript_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in file1:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        output.write(\"- \" + line + \"\\n\")\n",
    "\n",
    "file1.close()\n",
    "\n",
    "\n",
    "# image captions \n",
    "output.write(\"\\nImage Captions\\n\")\n",
    "file2 = open(captions_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in file2:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        output.write(\"- \" + line + \"\\n\")\n",
    "\n",
    "file2.close()\n",
    "\n",
    "\n",
    "\n",
    "# OCR text \n",
    "output.write(\"\\nOCR Text\\n\")\n",
    "file3 = open(ocr_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in file3:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        output.write(\"- \" + line + \"\\n\")\n",
    "\n",
    "file3.close()\n",
    "\n",
    "\n",
    "\n",
    "# Clips summary\n",
    "output.write(\"\\nFrame Summaries\\n\")\n",
    "file4 = open(summaries_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in file4:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        output.write(\"- \" + line + \"\\n\")\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee1b2f",
   "metadata": {},
   "source": [
    "### Multi-layer Prompting System\n",
    "- 3 descriptions \n",
    "- Rerank all desscriptions using CLIP model(on visual similarity to keyframes) and select only best one \n",
    "- Add contact info + hashtags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b44bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1:\n",
      "AI-enhanced photography with vivid painting effects. Live multi-screen mural display with rotating user content. Interactive quizzes and games.\n",
      "\n",
      "Candidate 2:\n",
      "AI-enhanced photography with vivid painting effects. Live multi-screen mural display with rotating user content. Interactive quizzes and games.\n",
      "\n",
      "Candidate 3:\n",
      "AI-enhanced photography with vivid painting effects. Live multi-screen mural display with rotating user content. Interactive quizzes and games.\n",
      "\n",
      "Score for candidate 1: 21.1954\n",
      "Score for candidate 2: 21.1954\n",
      "Score for candidate 3: 21.1954\n",
      "\n",
      "Selected candidate: 1\n",
      "\n",
      "Generating hashtags...\n",
      "\n",
      "Parsed hashtags: ['#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone', '#iphone']\n",
      "\n",
      "Appended contact info.\n",
      "\n",
      "Final description saved to: final_description.txt\n",
      "\n",
      "===== FINAL DESCRIPTION =====\n",
      "\n",
      "AI-enhanced photography with vivid painting effects. Live multi-screen mural display with rotating user content. Interactive quizzes and games.\n",
      "\n",
      "#iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone #iphone\n",
      "\n",
      "Contact: info@iboothme.com | Phone: +971 4 448 8563 | https://www.iboothme.com\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "facts_file       = \"facts/facts.txt\"\n",
    "keyframes_folder = \"keyframes\"\n",
    "output_file      = \"final_description.txt\"\n",
    "device           = torch.device(\"cpu\")\n",
    "# LLM Model\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer  = T5Tokenizer.from_pretrained(model_name)\n",
    "t5_model   = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "t5_model.eval()\n",
    "\n",
    "# Clip Model\n",
    "clip_name      = \"openai/clip-vit-base-patch32\"\n",
    "clip_model     = CLIPModel.from_pretrained(clip_name).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_name)\n",
    "clip_model.eval()\n",
    "\n",
    "with open(facts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    facts_text = f.read().strip()\n",
    "\n",
    "sample_examples = \"\"\"\n",
    "Sample 1:\n",
    "#DigitalArt #BrandActivation #PhotoBoothTechnology\n",
    "When a leading paint manufacturer challenged us to create an unforgettable brand experience, we delivered with the AI Mural Painting Photo Experience…\n",
    "Key Highlights\n",
    "• AI-enhanced photography with vivid painting effects\n",
    "• Interactive color selection tailored to brand palettes\n",
    "• Live multi-screen mural display with rotating user content\n",
    "Looking to transform your next event?\n",
    "\n",
    "Sample 2:\n",
    "Client: Huda Beauty  \n",
    "With the Claw Machine by iboothme, you can:\n",
    "• Create interactive experiences with branded quizzes and games\n",
    "• Distribute products intelligently while capturing valuable data\n",
    "Ready to take your brand activation to the next level?\n",
    "\n",
    "Sample 3:\n",
    "#AICapsule #capsuletrend #AIPhotoBooth\n",
    "Meet the AI Capsule — your new secret weapon for events…\n",
    "How it works:\n",
    "• Guests register and enter details\n",
    "• AI transforms images into a branded capsule, live\n",
    "Perfect for product launches, retail activations, experiential events.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "candidates = []\n",
    "\n",
    "for i in range(3):\n",
    "    prompt = (\n",
    "        \"Write a concise (around 150 words) marketing description in the style of these examples:\\n\"\n",
    "        + sample_examples\n",
    "        + \"\\nFACTS:\\n\"\n",
    "        + facts_text\n",
    "        + \"\\n\\nYOUR DESCRIPTION:\"\n",
    "    )\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = t5_model.generate(\n",
    "            input_ids=enc[\"input_ids\"],\n",
    "            attention_mask=enc[\"attention_mask\"],\n",
    "            max_new_tokens=100,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()\n",
    "    candidates.append(text)\n",
    "    print(f\"Candidate {i+1}:\\n{text}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Slecting the best\n",
    "\n",
    "images = []\n",
    "\n",
    "for fn in sorted(os.listdir(keyframes_folder)):\n",
    "\n",
    "    if fn.lower().endswith((\".jpg\", \".png\")):\n",
    "        images.append(Image.open(os.path.join(keyframes_folder, fn)).convert(\"RGB\"))\n",
    "\n",
    "    if len(images) == 3:\n",
    "        break\n",
    "\n",
    "scores = []\n",
    "\n",
    "if images:\n",
    "\n",
    "    for idx, desc in enumerate(candidates):\n",
    "        clip_in = clip_processor(\n",
    "            text=[desc],\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        ).to(device)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            score = clip_model(**clip_in).logits_per_text.mean().item()\n",
    "        scores.append(score)\n",
    "    best_index = scores.index(max(scores))\n",
    "\n",
    "else:\n",
    "    print(\"No keyframes found, defaulting to candidate 1\")\n",
    "    best_index = 0\n",
    "\n",
    "print(f\"\\nSelected candidate: {best_index+1}\\n\")\n",
    "final_description = candidates[best_index]\n",
    "\n",
    "\n",
    "\n",
    "# hashtags\n",
    "print(\"Generating hashtags...\\n\")\n",
    "tag_prompt = \"Extract 10-15 hashtags from this text:\\n\" + final_description\n",
    "enc = tokenizer(tag_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "with torch.no_grad():\n",
    "    tag_ids = t5_model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False\n",
    "    )\n",
    "raw_tags = tokenizer.decode(tag_ids[0], skip_special_tokens=True)\n",
    "all_tags = [t for t in raw_tags.replace(\",\", \" \").split() if t.startswith(\"#\")][:15]\n",
    "print(f\"Parsed hashtags: {all_tags}\\n\")\n",
    "if all_tags:\n",
    "    final_description += \"\\n\\n\" + \" \".join(all_tags)\n",
    "\n",
    "\n",
    "\n",
    "# contact info \n",
    "if \"info@iboothme.com\" not in final_description:\n",
    "    final_description += (\n",
    "        \"\\n\\nContact: info@iboothme.com | Phone: +971 4 448 8563 | https://www.iboothme.com\"\n",
    "    )\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_description)\n",
    "print(\"Final description saved :\", output_file)\n",
    "print()\n",
    "print(\"Final description\")\n",
    "print(final_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded description saved to: expanded_description.txt\n",
      "\n",
      "===== EXPANDED DESCRIPTION =====\n",
      "\n",
      "1. AI-enhanced photography with vivid painting effects. 2. Live multi-screen mural display with rotating user content. 3. Interactive quizzes and games. Each feature has a different purpose, but they all have one thing in common: They all have a big impact on marketing and brand image. The first feature focuses on AI. The second feature is a live mural display. The third feature is an interactive quiz and game. The final feature is the last one, but it's the most important one. It's about how it affects marketing, brand image, and what it does for the target audience. For example, the first feature is about how AI can be used to create a mural.\n",
      "\n",
      "Contact:\n",
      "- info@iboothme.com\n",
      "- +971 4 448 8563\n",
      "- https://www.iboothme.com\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "input_file  = \"final_description.txt\"\n",
    "output_file = \"expanded_description.txt\"\n",
    "device      = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer  = T5Tokenizer.from_pretrained(model_name)\n",
    "model      = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()\n",
    "\n",
    "sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "main_sentences = sentences[:3]\n",
    "\n",
    "prompt = (\n",
    "    \"You are a senior marketing copywriter.\\n\"\n",
    "    \"Below are three main feature sentences. For each one, write 2–3 sentences explaining:\\n\"\n",
    "    \"- Why this feature matters\\n\"\n",
    "    \"- How it impacts marketing and brand image\\n\"\n",
    "    \"- What it does for the target audience\\n\\n\"\n",
    ")\n",
    "for i, sent in enumerate(main_sentences, 1):\n",
    "    prompt += f\"{i}. {sent}.\\n\"\n",
    "\n",
    "prompt += \"\\nCompose these expansions into a single flowing marketing paragraph, no hashtags:\\n\\n\"\n",
    "\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "        min_length=150,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "expanded = tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "expanded += \"\\n\\nContact:\\n- info@iboothme.com\\n- +971 4 448 8563\\n- https://www.iboothme.com\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(expanded)\n",
    "\n",
    "print(\"Expanded description saved to:\", output_file)\n",
    "print(\"\\nEXPANDED DESCRIPTION \\n\")\n",
    "print(expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d07b7",
   "metadata": {},
   "source": [
    "### Fix to hastag bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9d57399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated hashtags: ['#feature', '#mural', '#display', '#interactive', '#marketing', '#brand', '#image', '#iboothme', '#enhanced', '#photography', '#vivid', '#painting', '#effects', '#multi', '#screen']\n",
      "KINDLYY SEE expanded_with_Hastags.txt for my Literally the final output\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"expanded_description.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "stopwords = {\n",
    "    \"about\",\"after\",\"again\",\"against\",\"among\",\"around\",\"because\",\"before\",\"being\",\n",
    "    \"below\",\"between\",\"both\",\"could\",\"during\",\"each\",\"first\",\"found\",\"from\",\"have\",\n",
    "    \"having\",\"however\",\"into\",\"other\",\"over\",\"through\",\"under\",\"while\",\"which\",\"your\",\n",
    "    \"their\",\"there\",\"where\",\"with\",\"this\",\"that\",\"these\",\"those\",\"would\",\"should\"\n",
    "}\n",
    "\n",
    "words = re.findall(r\"\\b[a-zA-Z]{5,}\\b\", text.lower())\n",
    "\n",
    "candidates = [w for w in words if w not in stopwords]\n",
    "most_common = Counter(candidates).most_common(15)\n",
    "hashtags = [f\"#{word}\" for word, count in most_common]\n",
    "final_text = text.strip() + \"\\n\\n\" + \" \".join(hashtags)\n",
    "\n",
    "with open(\"expanded_with_hashtags.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)\n",
    "\n",
    "print(\"Generated hashtags:\", hashtags)\n",
    "print(\"KINDLYY SEE expanded_with_Hastags.txt for my Literally the final output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a22369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
